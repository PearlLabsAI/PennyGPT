DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: ['chat.tcss']
DEBUG - ContextHeader updated
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Getting contents of files: ['chat.tcss']
DEBUG - Read content from chat.tcss
DEBUG - 

DEBUG - [92mRequest to litellm:[0m
DEBUG - [92mlitellm.completion(model='gpt-4', messages=[{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}], max_tokens=1000, temperature=0.2, stream=True)[0m
DEBUG - 

DEBUG - self.optional_params: {}
DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
DEBUG - 
LiteLLM completion() model= gpt-4; provider = openai
DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gpt-4', 'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': 1000, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'drop_params': None, 'additional_drop_params': None}
DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'stream': True, 'max_tokens': 1000}
DEBUG - Final returned optional params: {'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}
DEBUG - self.optional_params: {'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}
DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG - load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/penny/lib/python3.11/site-packages/certifi-2024.2.2-py3.11.egg/certifi/cacert.pem'
DEBUG - PRE-API-CALL ADDITIONAL ARGS: {'headers': {'Authorization': 'Bearer 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'}, 'api_base': ParseResult(scheme='https', userinfo='', host='api.openai.com', port=None, path='/v1/', query=None, fragment=None), 'acompletion': False, 'complete_input_dict': {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}], 'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}}
DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1/ \
-H 'Authorization: Bearer sk-pZpd********************************************' \
-d '{'model': 'gpt-4', 'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}], 'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}'
[0m

DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'timeout': 600.0, 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}], 'model': 'gpt-4', 'max_tokens': 1000, 'stream': True, 'temperature': 0.2}, 'extra_json': {}}
DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=600.0 socket_options=None
DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cd90690>
DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10c514320> server_hostname='api.openai.com' timeout=600.0
DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cd90990>
DEBUG - send_request_headers.started request=<Request [b'POST']>
DEBUG - send_request_headers.complete
DEBUG - send_request_body.started request=<Request [b'POST']>
DEBUG - send_request_body.complete
DEBUG - receive_response_headers.started request=<Request [b'POST']>
DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 17:38:00 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-jd0t5i0q5imxfr93bcksvmxv'), (b'openai-processing-ms', b'575'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'298745'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'250ms'), (b'x-request-id', b'req_e39e59a617e9cb437177f0d419ae04f8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Z0dysKmTBTDP7ikcWFZ1c6LCMNZVrNYDPnyOBzp.hkQ-1720373880-1.0.1.1-qNzeOiEAq4h9IbLOBhYruIUXs9uMQM9oD03vTL6pcZeZsXyCZdiWq.oJID5h3sdcjoM0a63lIKBfX1tq.9c2UA; path=/; expires=Sun, 07-Jul-24 18:08:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ct74U1zXiflbeilGtL7lpOMCkKpU2WGj7rtoal5I36s-1720373880655-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89f99b0c3db113b1-IAD'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Sun, 07 Jul 2024 17:38:00 GMT'), ('content-type', 'text/event-stream; charset=utf-8'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'user-jd0t5i0q5imxfr93bcksvmxv'), ('openai-processing-ms', '575'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '300000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '298745'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '250ms'), ('x-request-id', 'req_e39e59a617e9cb437177f0d419ae04f8'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Z0dysKmTBTDP7ikcWFZ1c6LCMNZVrNYDPnyOBzp.hkQ-1720373880-1.0.1.1-qNzeOiEAq4h9IbLOBhYruIUXs9uMQM9oD03vTL6pcZeZsXyCZdiWq.oJID5h3sdcjoM0a63lIKBfX1tq.9c2UA; path=/; expires=Sun, 07-Jul-24 18:08:00 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=ct74U1zXiflbeilGtL7lpOMCkKpU2WGj7rtoal5I36s-1720373880655-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89f99b0c3db113b1-IAD'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG - request_id: req_e39e59a617e9cb437177f0d419ae04f8
DEBUG - RAW RESPONSE:
<litellm.utils.CustomStreamWrapper object at 0x10cd67ed0>


DEBUG - receive_response_body.started request=<Request [b'POST']>
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: 
DEBUG - model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '', 'function_call': None, 'role': 'assistant', 'tool_calls': None}
DEBUG - new delta: Delta(content='', role='assistant', function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='', role='assistant', function_call=None, tool_calls=None); completion_obj: {'content': ''}
DEBUG - self.sent_first_chunk: False
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: Hello
DEBUG - model_response finish reason 3: None; response_obj={'text': 'Hello', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': 'Hello', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='Hello', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='Hello', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'Hello'}
DEBUG - self.sent_first_chunk: False
DEBUG - hold - False, model_response_str - Hello
DEBUG - choice_json: {'delta': {'content': 'Hello', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: False
DEBUG - model_response.choices[0].delta: Delta(content='Hello', role='assistant', function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: !
DEBUG - model_response finish reason 3: None; response_obj={'text': '!', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='!', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '!'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - !
DEBUG - choice_json: {'delta': {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content:  How
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' How', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' How', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' How', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' How', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' How'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  How
DEBUG - choice_json: {'delta': {'content': ' How', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' How', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  can
DEBUG - model_response finish reason 3: None; response_obj={'text': ' can', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' can', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - new delta: Delta(content=' can', role=None, function_call=None, tool_calls=None)
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content=' can', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' can'}
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  can
DEBUG - choice_json: {'delta': {'content': ' can', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' can', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  I
DEBUG - model_response finish reason 3: None; response_obj={'text': ' I', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' I'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  I
DEBUG - choice_json: {'delta': {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  assist
DEBUG - model_response finish reason 3: None; response_obj={'text': ' assist', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' assist', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' assist'}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - self.sent_first_chunk: True
DEBUG - Model=gpt-4;
DEBUG - hold - False, model_response_str -  assist
DEBUG - success callbacks: []
DEBUG - choice_json: {'delta': {'content': ' assist', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  you
DEBUG - model_response finish reason 3: None; response_obj={'text': ' you', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' you'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  you
DEBUG - choice_json: {'delta': {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content:  today
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' today', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' today', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' today', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' today', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' today'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  today
DEBUG - choice_json: {'delta': {'content': ' today', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' today', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ?
DEBUG - model_response finish reason 3: None; response_obj={'text': '?', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '?', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - new delta: Delta(content='?', role=None, function_call=None, tool_calls=None)
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content='?', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '?'}
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ?
DEBUG - choice_json: {'delta': {'content': '?', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='?', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: 
DEBUG - model_response finish reason 3: stop; response_obj={'text': '', 'is_finished': True, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373880, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=None, role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=None, role=None, function_call=None, tool_calls=None); completion_obj: {'content': ''}
DEBUG - self.sent_first_chunk: True
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ7kM0INRb5AQ8tSczAfFbwc0aTR', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373880, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Model=gpt-4;
DEBUG - Using selector: KqueueSelector
DEBUG - success callbacks: []
DEBUG - receive_response_body.complete
DEBUG - response_closed.started
DEBUG - response_closed.complete
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Goes into checking if chunk has hiddden created at param
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Chunks have a created at hidden param
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Chunks sorted
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - token_counter messages received: [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}]
DEBUG - Starting new HTTPS connection (1): huggingface.co:443
DEBUG - https://huggingface.co:443 "HEAD /gpt-4/resolve/main/tokenizer.json HTTP/1.1" 401 0
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Async success callbacks: Got a complete streaming response
DEBUG - completion_response response ms: 1343.694 
DEBUG - Looking up model=gpt-4 in model_cost_map
DEBUG - Success: model=gpt-4 in model_cost_map
DEBUG - prompt_tokens=325; completion_tokens=9
DEBUG - Returned custom cost for model=gpt-4 - prompt_tokens_cost_usd_dollar: 0.00975, completion_tokens_cost_usd_dollar: 0.00054
DEBUG - final cost: 0.01029; prompt_tokens_cost_usd_dollar: 0.00975; completion_tokens_cost_usd_dollar: 0.00054
DEBUG - Model=gpt-4; cost=0.01029
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - Goes into checking if chunk has hiddden created at param
DEBUG - Chunks have a created at hidden param
DEBUG - Chunks sorted
DEBUG - token_counter messages received: [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}]
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Logging Details LiteLLM-Success Call streaming complete
DEBUG - completion_response response ms: 2039.386 
DEBUG - Looking up model=gpt-4 in model_cost_map
DEBUG - Success: model=gpt-4 in model_cost_map
DEBUG - prompt_tokens=325; completion_tokens=9
DEBUG - Returned custom cost for model=gpt-4 - prompt_tokens_cost_usd_dollar: 0.00975, completion_tokens_cost_usd_dollar: 0.00054
DEBUG - final cost: 0.01029; prompt_tokens_cost_usd_dollar: 0.00975; completion_tokens_cost_usd_dollar: 0.00054
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Getting contents of files: ['chat.tcss']
DEBUG - Read content from chat.tcss
DEBUG - 

DEBUG - [92mRequest to litellm:[0m
DEBUG - [92mlitellm.completion(model='gpt-4', messages=[{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}], max_tokens=1000, temperature=0.2, stream=True)[0m
DEBUG - 

DEBUG - self.optional_params: {}
DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
DEBUG - 
LiteLLM completion() model= gpt-4; provider = openai
DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gpt-4', 'functions': None, 'function_call': None, 'temperature': 0.2, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': 1000, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'drop_params': None, 'additional_drop_params': None}
DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.2, 'stream': True, 'max_tokens': 1000}
DEBUG - Final returned optional params: {'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}
DEBUG - self.optional_params: {'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}
DEBUG - PRE-API-CALL ADDITIONAL ARGS: {'headers': {'Authorization': 'Bearer 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'}, 'api_base': ParseResult(scheme='https', userinfo='', host='api.openai.com', port=None, path='/v1/', query=None, fragment=None), 'acompletion': False, 'complete_input_dict': {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}], 'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}}
DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1/ \
-H 'Authorization: Bearer sk-pZpd********************************************' \
-d '{'model': 'gpt-4', 'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}], 'temperature': 0.2, 'stream': True, 'max_tokens': 1000, 'extra_body': {}}'
[0m

DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'timeout': 600.0, 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}], 'model': 'gpt-4', 'max_tokens': 1000, 'stream': True, 'temperature': 0.2}, 'extra_json': {}}
DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG - close.started
DEBUG - close.complete
DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=600.0 socket_options=None
DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10d8cd8d0>
DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10c514320> server_hostname='api.openai.com' timeout=600.0
DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cd720d0>
DEBUG - send_request_headers.started request=<Request [b'POST']>
DEBUG - send_request_headers.complete
DEBUG - send_request_body.started request=<Request [b'POST']>
DEBUG - send_request_body.complete
DEBUG - receive_response_headers.started request=<Request [b'POST']>
DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 17:38:20 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-jd0t5i0q5imxfr93bcksvmxv'), (b'openai-processing-ms', b'183'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'298725'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'254ms'), (b'x-request-id', b'req_8ed6fc4bd4938e750fec134226d99698'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89f99b8cc87981be-IAD'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 17:38:20 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-jd0t5i0q5imxfr93bcksvmxv', 'openai-processing-ms': '183', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '300000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '298725', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '254ms', 'x-request-id': 'req_8ed6fc4bd4938e750fec134226d99698', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89f99b8cc87981be-IAD', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG - request_id: req_8ed6fc4bd4938e750fec134226d99698
DEBUG - RAW RESPONSE:
<litellm.utils.CustomStreamWrapper object at 0x10cd5b590>


DEBUG - receive_response_body.started request=<Request [b'POST']>
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: 
DEBUG - model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '', 'function_call': None, 'role': 'assistant', 'tool_calls': None}
DEBUG - new delta: Delta(content='', role='assistant', function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='', role='assistant', function_call=None, tool_calls=None); completion_obj: {'content': ''}
DEBUG - self.sent_first_chunk: False
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='Yes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='Yes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: Yes
DEBUG - model_response finish reason 3: None; response_obj={'text': 'Yes', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='Yes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': 'Yes', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='Yes', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='Yes', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'Yes'}
DEBUG - self.sent_first_chunk: False
DEBUG - hold - False, model_response_str - Yes
DEBUG - choice_json: {'delta': {'content': 'Yes', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Yes', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: False
DEBUG - model_response.choices[0].delta: Delta(content='Yes', role='assistant', function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Yes', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Yes', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  the
DEBUG - success callbacks: []
DEBUG - model_response finish reason 3: None; response_obj={'text': ' the', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' the'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  the
DEBUG - choice_json: {'delta': {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' context', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' context', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  context
DEBUG - model_response finish reason 3: None; response_obj={'text': ' context', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' context', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' context', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' context', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' context', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' context'}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - self.sent_first_chunk: True
DEBUG - Model=gpt-4;
DEBUG - hold - False, model_response_str -  context
DEBUG - success callbacks: []
DEBUG - choice_json: {'delta': {'content': ' context', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' context', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' context', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' context', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' context', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' provided', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' provided', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  provided
DEBUG - model_response finish reason 3: None; response_obj={'text': ' provided', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' provided', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' provided', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' provided', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' provided', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' provided'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  provided
DEBUG - choice_json: {'delta': {'content': ' provided', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' provided', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' provided', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' provided', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' provided', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  is
DEBUG - success callbacks: []
DEBUG - model_response finish reason 3: None; response_obj={'text': ' is', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' is', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' is', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' is', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' is'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  is
DEBUG - choice_json: {'delta': {'content': ' is', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' is', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content:  a
DEBUG - model_response finish reason 3: None; response_obj={'text': ' a', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' a'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  a
DEBUG - choice_json: {'delta': {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373900, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  CSS
DEBUG - model_response finish reason 3: None; response_obj={'text': ' CSS', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' CSS'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  CSS
DEBUG - choice_json: {'delta': {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' (', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' (', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content:  (
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' (', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' (', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' (', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' (', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' (', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' ('}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  (
DEBUG - choice_json: {'delta': {'content': ' (', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' (', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' (', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' (', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' (', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='C', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='C', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: C
DEBUG - model_response finish reason 3: None; response_obj={'text': 'C', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='C', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': 'C', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='C', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='C', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'C'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - C
DEBUG - choice_json: {'delta': {'content': 'C', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='C', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='C', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='C', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='C', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='asc', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='asc', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: asc
DEBUG - model_response finish reason 3: None; response_obj={'text': 'asc', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='asc', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': 'asc', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='asc', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='asc', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'asc'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - asc
DEBUG - choice_json: {'delta': {'content': 'asc', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='asc', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='asc', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='asc', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='asc', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='ading', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='ading', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ading
DEBUG - model_response finish reason 3: None; response_obj={'text': 'ading', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='ading', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': 'ading', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='ading', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='ading', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'ading'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ading
DEBUG - choice_json: {'delta': {'content': 'ading', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='ading', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='ading', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='ading', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='ading', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - success callbacks: []
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  Style
DEBUG - model_response finish reason 3: None; response_obj={'text': ' Style', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' Style', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' Style', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' Style', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' Style'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  Style
DEBUG - choice_json: {'delta': {'content': ' Style', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Style', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' Style', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Style', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Style', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - success callbacks: []
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Sheets', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Sheets', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  Sheets
DEBUG - model_response finish reason 3: None; response_obj={'text': ' Sheets', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' Sheets', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' Sheets', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' Sheets', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' Sheets', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' Sheets'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  Sheets
DEBUG - choice_json: {'delta': {'content': ' Sheets', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Sheets', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' Sheets', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Sheets', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' Sheets', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=')', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=')', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: )
DEBUG - model_response finish reason 3: None; response_obj={'text': ')', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=')', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ')', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=')', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=')', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ')'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str - )
DEBUG - choice_json: {'delta': {'content': ')', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=')', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=')', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=')', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=')', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - success callbacks: []
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  file
DEBUG - model_response finish reason 3: None; response_obj={'text': ' file', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' file', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' file', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' file', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' file'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  file
DEBUG - choice_json: {'delta': {'content': ' file', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' file', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  for
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' for', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' for'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  for
DEBUG - choice_json: {'delta': {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  a
DEBUG - success callbacks: []
DEBUG - model_response finish reason 3: None; response_obj={'text': ' a', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' a'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  a
DEBUG - choice_json: {'delta': {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - success callbacks: []
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  chat
DEBUG - model_response finish reason 3: None; response_obj={'text': ' chat', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' chat'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  chat
DEBUG - choice_json: {'delta': {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content:  application
DEBUG - model_response finish reason 3: None; response_obj={'text': ' application', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' application', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' application', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' application', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' application'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  application
DEBUG - choice_json: {'delta': {'content': ' application', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' application', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Using selector: KqueueSelector
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: .
DEBUG - model_response finish reason 3: None; response_obj={'text': '.', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '.'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - .
DEBUG - choice_json: {'delta': {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  CSS
DEBUG - model_response finish reason 3: None; response_obj={'text': ' CSS', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' CSS'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  CSS
DEBUG - choice_json: {'delta': {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - self.sent_first_chunk: True
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  is
DEBUG - model_response finish reason 3: None; response_obj={'text': ' is', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' is', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' is', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content=' is', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' is'}
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  is
DEBUG - choice_json: {'delta': {'content': ' is', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' is', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' is', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  a
DEBUG - model_response finish reason 3: None; response_obj={'text': ' a', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' a'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  a
DEBUG - choice_json: {'delta': {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  style
DEBUG - model_response finish reason 3: None; response_obj={'text': ' style', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' style', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ' style', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=' style', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=' style', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' style'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  style
DEBUG - choice_json: {'delta': {'content': ' style', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' style', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' style', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' style', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' style', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' sheet', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' sheet', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  sheet
DEBUG - model_response finish reason 3: None; response_obj={'text': ' sheet', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' sheet', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' sheet', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' sheet', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' sheet', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' sheet'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  sheet
DEBUG - choice_json: {'delta': {'content': ' sheet', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' sheet', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' sheet', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' sheet', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' sheet', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' language', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' language', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  language
DEBUG - model_response finish reason 3: None; response_obj={'text': ' language', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' language', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' language', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' language', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' language', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' language'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  language
DEBUG - choice_json: {'delta': {'content': ' language', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' language', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' language', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' language', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' language', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' used', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' used', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  used
DEBUG - model_response finish reason 3: None; response_obj={'text': ' used', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' used', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' used', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' used', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' used', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' used'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  used
DEBUG - choice_json: {'delta': {'content': ' used', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' used', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' used', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' used', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' used', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  for
DEBUG - model_response finish reason 3: None; response_obj={'text': ' for', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' for'}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - self.sent_first_chunk: True
DEBUG - Model=gpt-4;
DEBUG - hold - False, model_response_str -  for
DEBUG - success callbacks: []
DEBUG - choice_json: {'delta': {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' describing', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' describing', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  describing
DEBUG - model_response finish reason 3: None; response_obj={'text': ' describing', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' describing', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' describing', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' describing', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' describing', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' describing'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  describing
DEBUG - choice_json: {'delta': {'content': ' describing', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' describing', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' describing', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' describing', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' describing', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  the
DEBUG - model_response finish reason 3: None; response_obj={'text': ' the', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' the'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  the
DEBUG - choice_json: {'delta': {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - success callbacks: []
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' look', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' look', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  look
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response finish reason 3: None; response_obj={'text': ' look', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' look', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Model=gpt-4;
DEBUG - original delta: {'content': ' look', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - success callbacks: []
DEBUG - new delta: Delta(content=' look', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' look', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' look'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  look
DEBUG - choice_json: {'delta': {'content': ' look', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' look', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' look', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' look', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' look', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  and
DEBUG - model_response finish reason 3: None; response_obj={'text': ' and', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' and', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' and', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' and', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' and'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  and
DEBUG - choice_json: {'delta': {'content': ' and', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' and', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - success callbacks: []
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' formatting', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' formatting', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  formatting
DEBUG - model_response finish reason 3: None; response_obj={'text': ' formatting', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' formatting', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' formatting', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' formatting', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' formatting', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' formatting'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  formatting
DEBUG - choice_json: {'delta': {'content': ' formatting', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' formatting', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' formatting', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' formatting', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' formatting', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  of
DEBUG - model_response finish reason 3: None; response_obj={'text': ' of', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' of', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - new delta: Delta(content=' of', role=None, function_call=None, tool_calls=None)
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content=' of', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' of'}
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  of
DEBUG - choice_json: {'delta': {'content': ' of', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' of', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  a
DEBUG - model_response finish reason 3: None; response_obj={'text': ' a', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' a'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  a
DEBUG - choice_json: {'delta': {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' document', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' document', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content:  document
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' document', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' document', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' document', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' document', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' document', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' document'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  document
DEBUG - choice_json: {'delta': {'content': ' document', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' document', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' document', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' document', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' document', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' written', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' written', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  written
DEBUG - model_response finish reason 3: None; response_obj={'text': ' written', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' written', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ' written', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=' written', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=' written', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' written'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  written
DEBUG - choice_json: {'delta': {'content': ' written', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' written', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' written', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' written', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' written', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' in', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' in', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  in
DEBUG - model_response finish reason 3: None; response_obj={'text': ' in', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' in', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' in', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' in', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' in', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' in'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  in
DEBUG - choice_json: {'delta': {'content': ' in', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' in', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' in', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' in', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' in', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' HTML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' HTML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  HTML
DEBUG - model_response finish reason 3: None; response_obj={'text': ' HTML', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' HTML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' HTML', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' HTML', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' HTML', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' HTML'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  HTML
DEBUG - choice_json: {'delta': {'content': ' HTML', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' HTML', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' HTML', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' HTML', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' HTML', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' or', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' or', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content:  or
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ' or', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' or', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ' or', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' or', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' or', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' or'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  or
DEBUG - choice_json: {'delta': {'content': ' or', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' or', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' or', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' or', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' or', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' XML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' XML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  XML
DEBUG - model_response finish reason 3: None; response_obj={'text': ' XML', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' XML', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' XML', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' XML', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' XML', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' XML'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  XML
DEBUG - choice_json: {'delta': {'content': ' XML', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' XML', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' XML', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' XML', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' XML', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - success callbacks: []
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: .
DEBUG - model_response finish reason 3: None; response_obj={'text': '.', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '.'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - .
DEBUG - choice_json: {'delta': {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' This', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' This', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  This
DEBUG - model_response finish reason 3: None; response_obj={'text': ' This', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' This', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' This', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' This', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' This', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' This'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  This
DEBUG - choice_json: {'delta': {'content': ' This', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' This', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' This', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' This', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' This', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - success callbacks: []
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' specific', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' specific', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  specific
DEBUG - model_response finish reason 3: None; response_obj={'text': ' specific', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' specific', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' specific', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' specific', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' specific', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' specific'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  specific
DEBUG - choice_json: {'delta': {'content': ' specific', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' specific', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' specific', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' specific', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' specific', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  CSS
DEBUG - model_response finish reason 3: None; response_obj={'text': ' CSS', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' CSS', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' CSS'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  CSS
DEBUG - choice_json: {'delta': {'content': ' CSS', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' CSS', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' CSS', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Using selector: KqueueSelector
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  file
DEBUG - success callbacks: []
DEBUG - model_response finish reason 3: None; response_obj={'text': ' file', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' file', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' file', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' file', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' file', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' file'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  file
DEBUG - choice_json: {'delta': {'content': ' file', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' file', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' file', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' includes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' includes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  includes
DEBUG - model_response finish reason 3: None; response_obj={'text': ' includes', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' includes', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' includes', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' includes', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' includes', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' includes'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  includes
DEBUG - choice_json: {'delta': {'content': ' includes', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' includes', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' includes', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' includes', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' includes', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' styles', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' styles', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  styles
DEBUG - model_response finish reason 3: None; response_obj={'text': ' styles', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' styles', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' styles', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' styles', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' styles', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' styles'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  styles
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - choice_json: {'delta': {'content': ' styles', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - Model=gpt-4;
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' styles', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' styles', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' styles', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' styles', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  for
DEBUG - model_response finish reason 3: None; response_obj={'text': ' for', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' for'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  for
DEBUG - choice_json: {'delta': {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373901, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' various', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' various', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  various
DEBUG - model_response finish reason 3: None; response_obj={'text': ' various', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' various', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' various', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' various', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' various', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' various'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  various
DEBUG - choice_json: {'delta': {'content': ' various', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' various', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' various', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' various', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' various', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' elements', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' elements', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  elements
DEBUG - model_response finish reason 3: None; response_obj={'text': ' elements', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' elements', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ' elements', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=' elements', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=' elements', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' elements'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  elements
DEBUG - choice_json: {'delta': {'content': ' elements', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' elements', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' elements', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' elements', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' elements', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  of
DEBUG - model_response finish reason 3: None; response_obj={'text': ' of', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' of', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' of', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' of', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' of'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  of
DEBUG - choice_json: {'delta': {'content': ' of', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' of', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' of', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content:  the
DEBUG - model_response finish reason 3: None; response_obj={'text': ' the', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' the'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  the
DEBUG - choice_json: {'delta': {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  chat
DEBUG - model_response finish reason 3: None; response_obj={'text': ' chat', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' chat'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  chat
DEBUG - choice_json: {'delta': {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  application
DEBUG - model_response finish reason 3: None; response_obj={'text': ' application', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' application', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' application', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' application', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' application', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' application'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  application
DEBUG - choice_json: {'delta': {'content': ' application', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' application', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' application', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' such', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' such', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  such
DEBUG - model_response finish reason 3: None; response_obj={'text': ' such', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' such', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' such', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' such', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' such', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' such'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  such
DEBUG - choice_json: {'delta': {'content': ' such', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' such', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' such', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' such', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' such', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' as', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' as', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  as
DEBUG - model_response finish reason 3: None; response_obj={'text': ' as', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' as', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' as', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' as', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' as', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' as'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  as
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - choice_json: {'delta': {'content': ' as', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - Model=gpt-4;
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' as', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' as', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' as', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' as', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  the
DEBUG - model_response finish reason 3: None; response_obj={'text': ' the', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - new delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - Model=gpt-4;
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' the'}
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  the
DEBUG - choice_json: {'delta': {'content': ' the', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' the', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' the', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' screen', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' screen', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  screen
DEBUG - model_response finish reason 3: None; response_obj={'text': ' screen', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' screen', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' screen', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' screen', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' screen', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' screen'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  screen
DEBUG - choice_json: {'delta': {'content': ' screen', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' screen', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' screen', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' screen', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' screen', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - Model=gpt-4;
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' header', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' header', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content:  header
DEBUG - model_response finish reason 3: None; response_obj={'text': ' header', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' header', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' header', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' header', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' header', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' header'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  header
DEBUG - choice_json: {'delta': {'content': ' header', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' header', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' header', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' header', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' header', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' footer', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' footer', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  footer
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response finish reason 3: None; response_obj={'text': ' footer', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' footer', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Model=gpt-4;
DEBUG - original delta: {'content': ' footer', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - success callbacks: []
DEBUG - new delta: Delta(content=' footer', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' footer', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' footer'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  footer
DEBUG - choice_json: {'delta': {'content': ' footer', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' footer', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' footer', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' footer', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' footer', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - success callbacks: []
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  chat
DEBUG - model_response finish reason 3: None; response_obj={'text': ' chat', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' chat'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  chat
DEBUG - choice_json: {'delta': {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - success callbacks: []
DEBUG - completion obj content:  input
DEBUG - model_response finish reason 3: None; response_obj={'text': ' input', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' input', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' input', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' input', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' input'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  input
DEBUG - choice_json: {'delta': {'content': ' input', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' input', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - hold - False, model_response_str - ,
DEBUG - Model=gpt-4;
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - success callbacks: []
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' main', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' main', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  main
DEBUG - model_response finish reason 3: None; response_obj={'text': ' main', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' main', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' main', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' main', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' main', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' main'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  main
DEBUG - choice_json: {'delta': {'content': ' main', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' main', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' main', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' main', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' main', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  container
DEBUG - model_response finish reason 3: None; response_obj={'text': ' container', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ' container', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=' container', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=' container', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' container'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  container
DEBUG - choice_json: {'delta': {'content': ' container', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' container', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content: ,
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  chat
DEBUG - model_response finish reason 3: None; response_obj={'text': ' chat', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' chat', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' chat'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  chat
DEBUG - choice_json: {'delta': {'content': ' chat', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' chat', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' chat', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' display', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' display', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  display
DEBUG - model_response finish reason 3: None; response_obj={'text': ' display', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' display', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' display', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' display', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' display', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' display'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  display
DEBUG - choice_json: {'delta': {'content': ' display', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' display', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' display', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' display', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' display', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Using selector: KqueueSelector
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ,
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Model=gpt-4;
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  input
DEBUG - model_response finish reason 3: None; response_obj={'text': ' input', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' input', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' input', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' input', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' input', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' input'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  input
DEBUG - choice_json: {'delta': {'content': ' input', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' input', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' input', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content:  container
DEBUG - model_response finish reason 3: None; response_obj={'text': ' container', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' container', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' container', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' container', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' container', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' container'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  container
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - choice_json: {'delta': {'content': ' container', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - success callbacks: []
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' container', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' container', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content: ,
DEBUG - model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - Model=gpt-4;
DEBUG - new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - success callbacks: []
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - ,
DEBUG - choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  and
DEBUG - model_response finish reason 3: None; response_obj={'text': ' and', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' and', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' and', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' and', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' and'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  and
DEBUG - choice_json: {'delta': {'content': ' and', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' and', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' and', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' send', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' send', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  send
DEBUG - model_response finish reason 3: None; response_obj={'text': ' send', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' send', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' send', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' send', role=None, function_call=None, tool_calls=None)
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - model_response.choices[0].delta: Delta(content=' send', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' send'}
DEBUG - Model=gpt-4;
DEBUG - self.sent_first_chunk: True
DEBUG - success callbacks: []
DEBUG - hold - False, model_response_str -  send
DEBUG - choice_json: {'delta': {'content': ' send', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' send', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' send', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' send', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' send', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' button', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' button', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - completion obj content:  button
DEBUG - model_response finish reason 3: None; response_obj={'text': ' button', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=' button', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': ' button', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=' button', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=' button', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' button'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str -  button
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - choice_json: {'delta': {'content': ' button', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - Model=gpt-4;
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' button', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - success callbacks: []
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content=' button', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' button', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' button', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Model=gpt-4;
DEBUG - completion obj content: .
DEBUG - success callbacks: []
DEBUG - model_response finish reason 3: None; response_obj={'text': '.', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - original delta: {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '.'}
DEBUG - self.sent_first_chunk: True
DEBUG - hold - False, model_response_str - .
DEBUG - choice_json: {'delta': {'content': '.', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}
DEBUG - choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)]
DEBUG - self.sent_first_chunk: True
DEBUG - model_response.choices[0].delta: Delta(content='.', role=None, function_call=None, tool_calls=None)
DEBUG - returning model_response: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='.', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4;
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai
DEBUG - 
Raw OpenAI Chunk
ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None)

DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - completion obj content: 
DEBUG - Model=gpt-4;
DEBUG - model_response finish reason 3: stop; response_obj={'text': '', 'is_finished': True, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1720373900, model='gpt-4-0613', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}
DEBUG - success callbacks: []
DEBUG - original delta: {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}
DEBUG - new delta: Delta(content=None, role=None, function_call=None, tool_calls=None)
DEBUG - model_response.choices[0].delta: Delta(content=None, role=None, function_call=None, tool_calls=None); completion_obj: {'content': ''}
DEBUG - self.sent_first_chunk: True
DEBUG - PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9iQ842QFq0QRoVmDAeXRqbHJ91hBE', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1720373902, model='gpt-4', object='chat.completion.chunk', system_fingerprint=None)
DEBUG - Using selector: KqueueSelector
DEBUG - Logging Details LiteLLM-Async Success Call
DEBUG - Model=gpt-4;
DEBUG - Goes into checking if chunk has hiddden created at param
DEBUG - receive_response_body.complete
DEBUG - Chunks have a created at hidden param
DEBUG - response_closed.started
DEBUG - Chunks sorted
DEBUG - response_closed.complete
DEBUG - token_counter messages received: [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}]
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Async success callbacks: Got a complete streaming response
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering code: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - completion_response response ms: 2586.754 
DEBUG - entering fence: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Looking up model=gpt-4 in model_cost_map
DEBUG - entering blockquote: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Success: model=gpt-4 in model_cost_map
DEBUG - entering hr: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - prompt_tokens=351; completion_tokens=81
DEBUG - entering list: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Returned custom cost for model=gpt-4 - prompt_tokens_cost_usd_dollar: 0.01053, completion_tokens_cost_usd_dollar: 0.00486
DEBUG - entering reference: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - final cost: 0.01539; prompt_tokens_cost_usd_dollar: 0.01053; completion_tokens_cost_usd_dollar: 0.00486
DEBUG - entering html_block: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering heading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering lheading: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - entering paragraph: StateBlock(line=0,level=0,tokens=0), 0, 2, False
DEBUG - Model=gpt-4; cost=0.01539
DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
DEBUG - Model=gpt-4;
DEBUG - success callbacks: []
DEBUG - Goes into checking if chunk has hiddden created at param
DEBUG - Chunks have a created at hidden param
DEBUG - Chunks sorted
DEBUG - token_counter messages received: [{'role': 'system', 'content': "Context:\nFile: chat.tcss\n/* chat.css */\nScreen {\n    background: #2e2e2e;\n}\n\nCustomHeader {\n    height: auto;\n    min-height: 3;\n    dock: top;\n}\n\nFooter {\n    height: 1;\n    dock: bottom;\n}\n\nChatInput {\n    background: #1e1e1e;\n    color: #fff;\n    border: solid #444;\n}\n\n#main_container {\n    height: 100%;\n    padding: 1 1 1 1; /* Use 1, 2, or 4 integers for padding */\n}\n\n#chat_display {\n    border: solid #ccc; /* Specify border type and color */\n    padding: 1; /* Use 1, 2, or 4 integers for padding */\n    margin-bottom: 1; /* Use 1, 2, or 4 integers for margin */\n    height: 70%; /* Use a valid unit for height */\n    overflow: auto;\n}\n\n#input_container {\n    align: center middle; /* Use 'align' with two values */\n}\n\n#send_button {\n    margin-left: 1; /* Use 1, 2, or 4 integers for margin */\n    padding: 1 2; /* Use 1, 2, or 4 integers for padding */\n    background: #007bff;\n    border: none;\n    border-right: solid #ccc; /* Use 'border-right' with a valid border type and color */\n}\n\n\n\n"}, {'role': 'user', 'content': 'Hmmm'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Do you have any context in your prompt?'}]
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Token Counter - using OpenAI token counter, for model=gpt-4
DEBUG - LiteLLM: Utils - Counting tokens for OpenAI model=gpt-4
DEBUG - Logging Details LiteLLM-Success Call streaming complete
DEBUG - completion_response response ms: 2601.29 
DEBUG - Looking up model=gpt-4 in model_cost_map
DEBUG - Success: model=gpt-4 in model_cost_map
DEBUG - prompt_tokens=351; completion_tokens=81
DEBUG - Returned custom cost for model=gpt-4 - prompt_tokens_cost_usd_dollar: 0.01053, completion_tokens_cost_usd_dollar: 0.00486
DEBUG - final cost: 0.01539; prompt_tokens_cost_usd_dollar: 0.01053; completion_tokens_cost_usd_dollar: 0.00486
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: {'chat.tcss'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: {'chat.tcss'}
DEBUG - Added file to context: chat.tcss
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: requirements.txt
DEBUG - Adding file to context: requirements.txt
DEBUG - Attempting to add file: requirements.txt
DEBUG - Updated context_files: {'requirements.txt'}
DEBUG - Added file to context: requirements.txt
DEBUG - Context files: {'requirements.txt', 'No files in context'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: {'chat.tcss'}
DEBUG - Added file to context: chat.tcss
DEBUG - Context files: {'No files in context', 'chat.tcss'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: {'chat.tcss'}
DEBUG - Added file to context: chat.tcss
DEBUG - Context files: {'chat.tcss'}
DEBUG - ContextHeader updated
DEBUG - File selected: src/penny.py
DEBUG - Adding file to context: src/penny.py
DEBUG - Attempting to add file: src/penny.py
DEBUG - Updated context_files: {'chat.tcss', 'src/penny.py'}
DEBUG - Added file to context: src/penny.py
DEBUG - Context files: {'chat.tcss', 'src/penny.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: icons8-search-100.png
DEBUG - File selected: ChatService.py
DEBUG - Adding file to context: ChatService.py
DEBUG - Attempting to add file: ChatService.py
DEBUG - Updated context_files: {'ChatService.py'}
DEBUG - Added file to context: ChatService.py
DEBUG - Context files: {'ChatService.py'}
DEBUG - ContextHeader updated
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'ChatService.py', 'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'ChatService.py', 'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: ChatApp.py
DEBUG - Adding file to context: ChatApp.py
DEBUG - Attempting to add file: ChatApp.py
DEBUG - Updated context_files: {'ChatApp.py'}
DEBUG - Added file to context: ChatApp.py
DEBUG - Context files: {'ChatApp.py'}
DEBUG - ContextHeader updated
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'ChatApp.py', 'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'ChatApp.py', 'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - Using selector: KqueueSelector
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: ChatApp.py
DEBUG - Adding file to context: ChatApp.py
DEBUG - Attempting to add file: ChatApp.py
DEBUG - Updated context_files: {'ChatApp.py'}
DEBUG - Added file to context: ChatApp.py
DEBUG - Context files: {'ChatApp.py'}
DEBUG - ContextHeader updated
DEBUG - File selected: FileManagerModal.py
DEBUG - Adding file to context: FileManagerModal.py
DEBUG - Attempting to add file: FileManagerModal.py
DEBUG - Updated context_files: {'FileManagerModal.py', 'ChatApp.py'}
DEBUG - Added file to context: FileManagerModal.py
DEBUG - Context files: {'FileManagerModal.py', 'ChatApp.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.log
DEBUG - File selected: chat.tcss
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - File selected: ChatMessage.py
DEBUG - Adding file to context: ChatMessage.py
DEBUG - Attempting to add file: ChatMessage.py
DEBUG - Updated context_files: {'ChatMessage.py', 'chat_llm.py'}
DEBUG - Added file to context: ChatMessage.py
DEBUG - Context files: {'ChatMessage.py', 'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat.tcss
DEBUG - Adding file to context: chat.tcss
DEBUG - Attempting to add file: chat.tcss
DEBUG - Updated context_files: {'chat.tcss'}
DEBUG - Added file to context: chat.tcss
DEBUG - Context files: {'chat.tcss'}
DEBUG - ContextHeader updated
DEBUG - File selected: ChatMessage.py
DEBUG - Adding file to context: ChatMessage.py
DEBUG - Attempting to add file: ChatMessage.py
DEBUG - Updated context_files: {'ChatMessage.py', 'chat.tcss'}
DEBUG - Added file to context: ChatMessage.py
DEBUG - Context files: {'ChatMessage.py', 'chat.tcss'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: chat_llm.py
DEBUG - Adding file to context: chat_llm.py
DEBUG - Attempting to add file: chat_llm.py
DEBUG - Updated context_files: {'chat_llm.py'}
DEBUG - Added file to context: chat_llm.py
DEBUG - Context files: {'chat_llm.py'}
DEBUG - ContextHeader updated
DEBUG - File selected: chat_llm.py
DEBUG - File selected: ChatApp.py
DEBUG - Adding file to context: ChatApp.py
DEBUG - Attempting to add file: ChatApp.py
DEBUG - Updated context_files: {'chat_llm.py', 'ChatApp.py'}
DEBUG - Added file to context: ChatApp.py
DEBUG - Context files: {'chat_llm.py', 'ChatApp.py'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
DEBUG - File selected: README.md
DEBUG - Adding file to context: README.md
DEBUG - Attempting to add file: README.md
DEBUG - Updated context_files: {'README.md'}
DEBUG - Added file to context: README.md
DEBUG - Context files: {'README.md'}
DEBUG - ContextHeader updated
DEBUG - Using selector: KqueueSelector
